{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/matthewashman/github/MasterProject2018')\n",
    "\n",
    "# Import necessary modules. Set settings. Import data.\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pywt\n",
    "import fastdtw\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.robust import mad\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from FeatureExtraction.feature_tools import detect_peaks\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import pdb\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "X = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/extracted_segments_with_labels_updated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_S1 = X[X['S1/S2']=='S1']\n",
    "X_S2 = X[X['S1/S2']=='S2']\n",
    "X_S2 = X_S2[(X_S2['Label']=='0') |(X_S2['Label']=='1') | (X_S2['Label']=='2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features to be Used In Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_list = []\n",
    "\n",
    "for i,row in X_S2.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Features: ' + str(round((i/X_S2.index[-1])*100, 3)) + '%')\n",
    "    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    if (((row['Type'] + row['Patient']) == 'af8') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[2]\n",
    "    elif (((row['Type'] + row['Patient']) == 'at1') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[4]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt10') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[1]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avrt13') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = X_S2[(X_S2['Type']==row['Type']) & \n",
    "                           (X_S2['Patient']==row['Patient']) &\n",
    "                           (X_S2['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'af14') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = X_S2[(X_S2['Type']==row['Type']) & \n",
    "                           (X_S2['Patient']==row['Patient']) &\n",
    "                           (X_S2['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                               (X_S1['Patient']==row['Patient']) &\n",
    "                               (X_S1['Channel']==row['Channel'])\n",
    "                               ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "    \n",
    "    ref_feature_dict = get_feature_dict(s1_response, col_prefix = '')\n",
    "    feature_dict = get_feature_dict(s2_response, col_prefix = '')\n",
    "    \n",
    "    for k, v in feature_dict.items():\n",
    "        feature_dict[k] = v - ref_feature_dict[k]\n",
    "        \n",
    "    \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    feature_list.append(feature_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(feature_list)\n",
    "\n",
    "feature_values = features.drop(['Channel', 'Coupling Interval', 'Data', 'Label', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "info = features[['Channel', 'Coupling Interval', 'Data', 'Label', 'Patient', 'Type', 'S1/S2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "feature_pc = pca.fit_transform(feature_values.values)\n",
    "pca_df = pd.DataFrame(data = feature_pc,\n",
    "                                 columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "final_df = pd.concat([pca_df, info.reset_index()], axis = 1)\n",
    "\n",
    "labels = ['2', '1','0']\n",
    "colors = ['r', 'm', 'g']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "for label, color in zip(labels[::-1], colors[::-1]):\n",
    "    idx_to_keep = (final_df['Label'].values == label)\n",
    "    plt.scatter(final_df.loc[idx_to_keep, 'principal component 1']\n",
    "                   ,final_df.loc[idx_to_keep, 'principal component 2']\n",
    "                   ,c = color\n",
    "                   ,edgecolor='k'\n",
    "                   ,s = 50)\n",
    "   \n",
    "names = []\n",
    "for i, row in final_df.iterrows():\n",
    "    row_name = row['Type'] + row['Patient'] + ' ' + row['Channel'] + ' ' + row['Coupling Interval'] + ' ' + str(row['Label'])\n",
    "    names.append(row_name)\n",
    "    \n",
    "sc = plt.scatter(final_df['principal component 1'], final_df['principal component 2'],\n",
    "                alpha=0,\n",
    "                s=50)\n",
    "        \n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "annot.set_visible(False)\n",
    "\n",
    "def update_annot(ind):\n",
    "\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}\".format(\" \".join([names[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "                \n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "plt.legend(['Red', 'Amber', 'Green'])\n",
    "plt.grid(True)\n",
    "plt.title('PCA Project of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Projection Including Augmented Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_features_for_lda.pkl')\n",
    "augmented_features = X_aug[X_aug['Augmented']==1]\n",
    "\n",
    "augmented_feature_values = augmented_features.drop(['Augmented', 'Channel', 'Coupling Interval', 'Data', 'Label', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "augmented_info = augmented_features[['Augmented', 'Channel', 'Coupling Interval', 'Data', 'Label', 'Patient', 'Type', 'S1/S2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "lda = LDA(n_components=2)\n",
    "feature_values_lda = lda.fit_transform(feature_values.values, info['Label'])\n",
    "augmented_feature_values_lda = lda.transform(augmented_feature_values.values)\n",
    "lda_df = pd.DataFrame(data = feature_values_lda,\n",
    "                      columns = ['principal component 1', 'principal component 2'])\n",
    "augmented_lda_df = pd.DataFrame(data = augmented_feature_values_lda,\n",
    "                                columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "final_df = pd.concat([lda_df, info.reset_index()], axis = 1)\n",
    "augmented_final_df = pd.concat([augmented_lda_df, augmented_info.reset_index()], axis=1)\n",
    "\n",
    "labels = ['2', '1','0']\n",
    "colors = ['r', 'orange', 'g']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "for label, color in zip(labels[::-1], colors[::-1]):\n",
    "    idx_to_keep = (final_df['Label'].values == label)\n",
    "    plt.scatter(final_df.loc[idx_to_keep, 'principal component 1']\n",
    "                   ,final_df.loc[idx_to_keep, 'principal component 2']\n",
    "                   ,c = color\n",
    "                   ,edgecolor='k'\n",
    "                   ,s = 50)\n",
    "   \n",
    "plt.scatter(augmented_final_df['principal component 1'].values, augmented_final_df['principal component 2'].values,\n",
    "           c='r', \n",
    "           edgecolor='k',\n",
    "           s=50,\n",
    "           marker='v')\n",
    "names = []\n",
    "for i, row in final_df.iterrows():\n",
    "    row_name = row['Type'] + row['Patient'] + ' ' + row['Channel'] + ' ' + row['Coupling Interval'] + ' ' + str(row['Label'])\n",
    "    names.append(row_name)\n",
    "    \n",
    "sc = plt.scatter(final_df['principal component 1'], final_df['principal component 2'],\n",
    "                alpha=0,\n",
    "                s=50)\n",
    "        \n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "annot.set_visible(False)\n",
    "\n",
    "def update_annot(ind):\n",
    "\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}\".format(\" \".join([names[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "                \n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "plt.legend(['Green', 'Amber', 'Red', 'Augmented'], fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('LDA Projection of Features w/ Augmentation', fontsize=16)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "lda = LDA(n_components=2)\n",
    "normalised_feature_values = preprocessing.scale(feature_values.values)\n",
    "feature_values_lda = lda.fit_transform(normalised_feature_values, info['Label'])\n",
    "lda_df = pd.DataFrame(data = feature_values_lda,\n",
    "                      columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "final_df = pd.concat([lda_df, info.reset_index()], axis = 1)\n",
    "\n",
    "labels = ['2', '1','0']\n",
    "colors = ['r', 'orange', 'g']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "for label, color in zip(labels[::-1], colors[::-1]):\n",
    "    idx_to_keep = (final_df['Label'].values == label)\n",
    "    plt.scatter(final_df.loc[idx_to_keep, 'principal component 1']\n",
    "                   ,final_df.loc[idx_to_keep, 'principal component 2']\n",
    "                   ,c = color\n",
    "                   ,edgecolor='k'\n",
    "                   ,s = 50)\n",
    "   \n",
    "names = []\n",
    "for i, row in final_df.iterrows():\n",
    "    row_name = row['Type'] + row['Patient'] + ' ' + row['Channel'] + ' ' + row['Coupling Interval'] + ' ' + str(row['Label'])\n",
    "    names.append(row_name)\n",
    "    \n",
    "sc = plt.scatter(final_df['principal component 1'], final_df['principal component 2'],\n",
    "                alpha=0,\n",
    "                s=50)\n",
    "        \n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "annot.set_visible(False)\n",
    "\n",
    "def update_annot(ind):\n",
    "\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}\".format(\" \".join([names[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "                \n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "plt.legend(['Green', 'Amber', 'Red'], fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('LDA Projection of Features', fontsize=16)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A shitty conduction delay detector\n",
    "def get_delay(x, amp_thresh=None, set_thresh=False):\n",
    "    if (set_thresh==True):\n",
    "        if any(abs(x)>amp_thresh):\n",
    "            return np.argmax(abs(x)>amp_thresh)\n",
    "        else:\n",
    "            return len(x)\n",
    "    else:    \n",
    "        return np.argmax(abs(x)>(max(abs(x))/2))\n",
    "    \n",
    "def denoise(x):\n",
    "    # Obtain Daubechies N=6 wavelet coefficients\n",
    "    waveletCoefs = pywt.wavedec(x, 'db7', mode='per')\n",
    "\n",
    "    # Throw away coefficients corresponding to noise\n",
    "    sigma = mad(waveletCoefs[-1])\n",
    "    uThresh = 1*sigma*np.sqrt(2*np.log(len(x)))\n",
    "    denoised = waveletCoefs[:]\n",
    "    denoised[1:] = (pywt._thresholding.hard(i, value=uThresh) for i in denoised[1:])\n",
    "\n",
    "    # Reconstruct the original signal\n",
    "    xDenoised = pywt.waverec(denoised, 'db7', mode='per')\n",
    "\n",
    "    return xDenoised\n",
    "\n",
    "def get_peaks(x, height_thresh, scale_amp=None, set_scale=False, plot = False):\n",
    "    x = np.array(x)\n",
    "    \n",
    "    # Get height_thresh\n",
    "    if set_scale:\n",
    "        height_thresh = height_thresh*scale_amp\n",
    "    else:\n",
    "        height_thresh = height_thresh*max(abs(x))\n",
    "    \n",
    "    # Denoise x\n",
    "    xdn = denoise(x)\n",
    "\n",
    "    # Detect peaks using detect_peaks\n",
    "    pos_peak_idx = detect_peaks(xdn, mph=height_thresh, threshold = 0)\n",
    "    neg_peak_idx = detect_peaks((-xdn), mph=height_thresh, threshold = 0)\n",
    "    peak_idx = np.concatenate([pos_peak_idx, neg_peak_idx])\n",
    "    peak_idx = np.sort(peak_idx)\n",
    "    # Edge indeces aren't detected\n",
    "    peak_idx = peak_idx[(peak_idx != 0) & (peak_idx != (len(xdn)-1))]\n",
    "\n",
    "    new_peak_idx = []\n",
    "    peak_amp = []\n",
    "    if (len(peak_idx) > 0):\n",
    "        new_peak_idx.append(peak_idx[0])\n",
    "        mp_thresh = 0.2*max(abs(x))\n",
    "        for i in range(len(peak_idx)-1):\n",
    "            idx = peak_idx[i]\n",
    "            idx_next = peak_idx[i+1]\n",
    "            mid_point = int((idx_next+idx)/2)\n",
    "            if (max([abs(x[idx_next]-x[mid_point]), abs(x[idx]-x[mid_point])]) > mp_thresh):\n",
    "                new_peak_idx.append(idx_next)\n",
    "\n",
    "        peak_idx = np.array(new_peak_idx)\n",
    "        peak_amp = x[peak_idx]\n",
    "\n",
    "    if plot == True:\n",
    "        fig, [ax1] = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(8,8))\n",
    "        ax1.plot(x, 'b' , xdn, 'r--', peak_idx, peak_amp, 'kx')\n",
    "        #plt.title(fileName)\n",
    "        ax1.set_xlabel('Sample')\n",
    "        ax1.set_ylabel('Normalised amplitude')\n",
    "        ax1.legend(['Original segment', 'Denoised segment', 'Detected peaks'])\n",
    "\n",
    "        plt.draw()\n",
    "        plt.waitforbuttonpress(0) # this will wait for indefinite time\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    return peak_idx, peak_amp\n",
    "\n",
    "def sample_entropy(U, m, r):\n",
    "\n",
    "    def _maxdist(x_i, x_j):\n",
    "        result = max([abs(ua-va) for ua, va in zip(x_i, x_j)])\n",
    "        return result\n",
    "\n",
    "    def _phi(m):\n",
    "        x = np.zeros([N,m-1])\n",
    "        for i in range(N-m+1):\n",
    "            x[i,:] = U[i:i+m-1]\n",
    "\n",
    "        C = 0\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                if i != j:\n",
    "                    if _maxdist(x[i,:], x[j,:]) <= r:\n",
    "                        C = C + 1\n",
    "\n",
    "        return C\n",
    "\n",
    "    U = U/max(abs(U))\n",
    "    N = len(U)\n",
    "\n",
    "    return -np.log(_phi(m+1)/_phi(m))\n",
    "\n",
    "def percentage_fractionation(x, peak_idxs, thresh=0.01, sr=1000):\n",
    "    # Get peak indexes and amplitude\n",
    "    peak_idx_diffs = np.diff(peak_idxs)\n",
    "    frac_time = 0\n",
    "    frac_time = np.sum(peak_idx_diffs[peak_idx_diffs < thresh*sr])\n",
    "    prcnt_frac = (frac_time/len(x))*100\n",
    "    return prcnt_frac\n",
    "\n",
    "def get_local_sample_entropy(x, centre_idx, width, m=2, r=0.05):\n",
    "    # Ensure width is odd\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return sample_entropy(x[:width+1], m, r)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return sample_entropy(x[len(x)-1-width:], m, r)\n",
    "    else:\n",
    "        return sample_entropy(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)], m, r)\n",
    "    \n",
    "def get_location_of_max_energy(x, M=14):\n",
    "    v = np.ones(M)\n",
    "    x_ = np.convolve(abs(x), v)\n",
    "    return (np.argmax(x_) + math.floor(M/2))\n",
    "        \n",
    "def get_local_peaks(x, centre_idx, width=25, height_thresh=0.1):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_peaks(x[:width+1], height_thresh)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_peaks(x[len(x)-1-width:], height_thresh)\n",
    "    else:\n",
    "        return get_peaks(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)], height_thresh)\n",
    "    \n",
    "def get_pse(x):\n",
    "    x_fft = np.fft.rfft(x)\n",
    "    x_P = (1/len(x_fft))*np.absolute(x_fft)**2\n",
    "    x_p = x_P/sum(x_P)\n",
    "    pse = np.sum([(-p*np.log2(p)) for p in x_p])\n",
    "    return pse\n",
    "\n",
    "def get_local_pse(x, centre_idx, width=50):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_pse(x[:width+1])\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_pse(x[len(x)-1-width:])\n",
    "    else:\n",
    "        return get_pse(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)])\n",
    "    \n",
    "def get_spectral_centroid(x):\n",
    "    x_fft = np.fft.rfft(x)\n",
    "    x_spectrum = np.absolute(x_fft)\n",
    "    normalized_spectrum = x_spectrum/sum(x_spectrum)\n",
    "    normalized_frequencies = np.arange(0, len(x_spectrum), 1)\n",
    "    return sum(normalized_frequencies * normalized_spectrum)\n",
    "\n",
    "def get_local_spectral_centroid(x, centre_idx, width=50):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_spectral_centroid(x[:width+1])\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_spectral_centroid(x[len(x)-1-width:])\n",
    "    else:\n",
    "        return get_spectral_centroid(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)])\n",
    "    \n",
    "def get_local_energy(x, centre_idx, width=60):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return np.sum(x[:width+1]**2)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return np.sum(x[len(x)-1-width:]**2)\n",
    "    else:\n",
    "        return np.sum(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)]**2)\n",
    "    \n",
    "def get_width_max_energy(x, M=14, width_thresh=0.2):\n",
    "    v = np.ones(M)\n",
    "    x_ = np.convolve(abs(x), v)\n",
    "    if any(x_[np.argmax(x_):] < width_thresh*np.max(x_)):\n",
    "        end_idx = np.argmax(x_) + np.argmax(x_[np.argmax(x_):] < width_thresh*np.max(x_))\n",
    "    else:\n",
    "        end_idx = len(x_)-1\n",
    "    if any(x_[np.argmax(x_)::-1] < width_thresh*np.max(x_)):  \n",
    "        start_idx = np.argmax(x_) - np.argmax(x_[np.argmax(x_)::-1] < width_thresh*np.max(x_))\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    return (end_idx - start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(x, col_prefix=''):\n",
    "    feature_dict = {}\n",
    "    height_thresh=0.1\n",
    "        \n",
    "    # Hand engineered features\n",
    "    peaks = get_peaks(x, height_thresh)\n",
    "    feature_dict[col_prefix + 'Number of Peaks'] = len(peaks[0])\n",
    "    feature_dict[col_prefix + 'Percentage Fractionation'] = percentage_fractionation(x, peaks[0], thresh=0.01)\n",
    "    \n",
    "    \n",
    "    max_energy_idx = get_location_of_max_energy(x)\n",
    "    feature_dict[col_prefix + 'Location of Maximum Energy'] = max_energy_idx\n",
    "    feature_dict[col_prefix + 'Sample Entropy Around Max Energy'] = get_local_sample_entropy(x, max_energy_idx, 30, m=3, r=0.15)\n",
    "    feature_dict[col_prefix + 'Width of Maximum Energy'] = get_width_max_energy(x, M=14, width_thresh=0.2)\n",
    "    \n",
    "    # Temporal features\n",
    "    feature_dict[col_prefix + 'Ratio Above 1xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 1)\n",
    "    feature_dict[col_prefix + 'Mean Absolute Value'] = np.mean(abs(x)/max(abs(x)))\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
