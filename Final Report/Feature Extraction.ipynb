{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/matthewashman/github/MasterProject2018')\n",
    "\n",
    "# Import necessary modules. Set settings. Import data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import math\n",
    "from IPython.display import HTML\n",
    "\n",
    "# For model building\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, recall_score, make_scorer\n",
    "from sklearn import svm, naive_bayes, neighbors, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For feature extraction\n",
    "from scipy.interpolate import CubicSpline      # for warping\n",
    "from statsmodels.robust import mad\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from FeatureExtraction.feature_tools import detect_peaks\n",
    "from sklearn.utils import resample\n",
    "import fastdtw\n",
    "\n",
    "# Miscelaneous\n",
    "from IPython.display import display, clear_output\n",
    "import pdb\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "training_data = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/training_data.pkl')\n",
    "validation_data = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/validation_data.pkl')\n",
    "test_data = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/test_data.pkl')\n",
    "augmented_training_data_01 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_training_data_1.pkl')\n",
    "augmented_training_data_02 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_training_data_2.pkl')\n",
    "augmented_training_data_03 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_training_data_3.pkl')\n",
    "augmented_training_data_04 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_training_data_4.pkl')\n",
    "augmented_validation_data_01 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_validation_data_1.pkl')\n",
    "augmented_validation_data_02 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_validation_data_2.pkl')\n",
    "augmented_validation_data_03 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_validation_data_3.pkl')\n",
    "augmented_validation_data_04 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_validation_data_4.pkl')\n",
    "augmented_test_data_03 = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/augmented_test_data_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12' '13' '14' '15' '17' '18' '19']\n"
     ]
    }
   ],
   "source": [
    "print(augmented_test_data_03[augmented_test_data_03['Type']=='af']['Patient'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_training_data = training_data[training_data['S1/S2']=='S2']\n",
    "S1_training_data = training_data[training_data['S1/S2']=='S1']\n",
    "S2_validation_data = validation_data[validation_data['S1/S2']=='S2']\n",
    "S1_validation_data = validation_data[validation_data['S1/S2']=='S1']\n",
    "S2_test_data = test_data[test_data['S1/S2']=='S2']\n",
    "S1_test_data = test_data[test_data['S1/S2']=='S1']\n",
    "\n",
    "S2_augmented_training_data_01 = augmented_training_data_01[augmented_training_data_01['S1/S2']=='S2']\n",
    "S1_augmented_training_data_01 = augmented_training_data_01[augmented_training_data_01['S1/S2']=='S1']\n",
    "S2_augmented_training_data_02 = augmented_training_data_02[augmented_training_data_02['S1/S2']=='S2']\n",
    "S1_augmented_training_data_02 = augmented_training_data_02[augmented_training_data_02['S1/S2']=='S1']\n",
    "S2_augmented_training_data_03 = augmented_training_data_03[augmented_training_data_03['S1/S2']=='S2']\n",
    "S1_augmented_training_data_03 = augmented_training_data_03[augmented_training_data_03['S1/S2']=='S1']\n",
    "S2_augmented_training_data_04 = augmented_training_data_04[augmented_training_data_04['S1/S2']=='S2']\n",
    "S1_augmented_training_data_04 = augmented_training_data_04[augmented_training_data_04['S1/S2']=='S1']\n",
    "\n",
    "S2_augmented_validation_data_01 = augmented_validation_data_01[augmented_validation_data_01['S1/S2']=='S2']\n",
    "S1_augmented_validation_data_01 = augmented_validation_data_01[augmented_validation_data_01['S1/S2']=='S1']\n",
    "S2_augmented_validation_data_02 = augmented_validation_data_02[augmented_validation_data_02['S1/S2']=='S2']\n",
    "S1_augmented_validation_data_02 = augmented_validation_data_02[augmented_validation_data_02['S1/S2']=='S1']\n",
    "S2_augmented_validation_data_03 = augmented_validation_data_03[augmented_validation_data_03['S1/S2']=='S2']\n",
    "S1_augmented_validation_data_03 = augmented_validation_data_03[augmented_validation_data_03['S1/S2']=='S1']\n",
    "S2_augmented_validation_data_04 = augmented_validation_data_04[augmented_validation_data_04['S1/S2']=='S2']\n",
    "S1_augmented_validation_data_04 = augmented_validation_data_04[augmented_validation_data_04['S1/S2']=='S1']\n",
    "\n",
    "S2_augmented_test_data_03 = augmented_test_data_03[augmented_test_data_03['S1/S2']=='S2']\n",
    "S1_augmented_test_data_03 = augmented_test_data_03[augmented_test_data_03['S1/S2']=='S1']\n",
    "\n",
    "S2_data = pd.concat([S2_training_data, S2_validation_data, S2_test_data], axis=0, ignore_index=True)\n",
    "S1_data = pd.concat([S2_training_data, S2_validation_data, S2_test_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Training Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_feature_list = []\n",
    "\n",
    "for i,row in S2_training_data.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Training Features: ' + str(round(100*i/S2_training_data.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train_feature_list)\n",
    "X_train.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Validation Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_validation_feature_list = []\n",
    "\n",
    "for i,row in S2_validation_data.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Validation Features: ' + str(round(100*i/S2_validation_data.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    try:\n",
    "        typical_response = get_typical_response(row)\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "        print('WTF')\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_validation_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = pd.DataFrame(X_validation_feature_list)\n",
    "X_validation.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_validation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Augmented Training Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_aug_01_feature_list = []\n",
    "\n",
    "for i,row in S2_augmented_training_data_01.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Augmented Training Features: ' + str(round(100*i/S2_augmented_training_data_01.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "    \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_aug_01_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a = X_train.copy()\n",
    "X_train_a['Augmented'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented_01 = pd.DataFrame(X_train_aug_01_feature_list)\n",
    "X_augmented_01 = pd.concat([X_augmented_01, X_train_a], axis=0, ignore_index=True)\n",
    "X_augmented_01.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_augmented_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Augmented Training Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_aug_02_feature_list = []\n",
    "\n",
    "for i,row in S2_augmented_training_data_02.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Augmented Training Features: ' + str(round(100*i/S2_augmented_training_data_02.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_aug_02_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented_02 = pd.DataFrame(X_train_aug_02_feature_list)\n",
    "X_augmented_02 = pd.concat([X_augmented_02, X_train_a], axis=0, ignore_index=True)\n",
    "X_augmented_02.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_augmented_02.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Augmented Training Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_aug_03_feature_list = []\n",
    "\n",
    "for i,row in S2_augmented_training_data_03.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Augmented Training Features: ' + str(round(100*i/S2_augmented_training_data_03.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_aug_03_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented_03 = pd.DataFrame(X_train_aug_03_feature_list)\n",
    "X_augmented_03 = pd.concat([X_augmented_03, X_train_a], axis=0, ignore_index=True)\n",
    "X_augmented_03.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_augmented_03.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Augmented Training Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_aug_04_feature_list = []\n",
    "\n",
    "for i,row in S2_augmented_training_data_04.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Augmented Training Features: ' + str(round(100*i/S2_augmented_training_data_04.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_aug_04_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented_04 = pd.DataFrame(X_train_aug_04_feature_list)\n",
    "X_augmented_04 = pd.concat([X_augmented_04, X_train_a], axis=0, ignore_index=True)\n",
    "X_augmented_04.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_augmented_04.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Augmented Validation Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_val_aug_03_feature_list = []\n",
    "\n",
    "for i,row in S2_augmented_validation_data_03.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Augmented Validation Features: ' + str(round(100*i/S2_augmented_validation_data_03.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_val_aug_03_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_a = X_validation.copy()\n",
    "X_validation_a['Augmented'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_augmented_03 = pd.DataFrame(X_val_aug_03_feature_list)\n",
    "X_validation_augmented_03 = pd.concat([X_validation_augmented_03, X_validation_a], axis=0, ignore_index=True)\n",
    "X_validation_augmented_03.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_validation_augmented_03.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Test Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_feature_list = []\n",
    "\n",
    "for i,row in S2_test_data.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Test Features: ' + str(round(100*i/S2_test_data.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_test_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test_feature_list)\n",
    "X_test.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Augmented Test Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_aug_03_feature_list = []\n",
    "\n",
    "for i,row in S2_augmented_test_data_03.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Augmented Test Features: ' + str(round(100*i/S2_augmented_test_data_03.index[-1],3)) + '%')    \n",
    "    # Get the patients response to the first S1 stimuli as the reference response\n",
    "    # Get typical response for this patient and channel\n",
    "    # Bad apples\n",
    "    typical_response = get_typical_response(row)\n",
    "        \n",
    "    # Normalise amplitudes with respect to the typical response amplitude.\n",
    "    s1_response = typical_response['Data']/max(abs(typical_response['Data']))\n",
    "    s2_response = row['Data']/max(abs(typical_response['Data']))\n",
    "        \n",
    "    ref_feature_dict = get_feature_dict(s1_response)\n",
    "    feature_dict = get_feature_dict(s2_response)\n",
    "    \n",
    "    for k, v in list(feature_dict.items()):\n",
    "        feature_dict[k + ' 2'] = v - ref_feature_dict[k]\n",
    "        \n",
    "    fdtw = fastdtw.dtw(s2_response, s1_response)\n",
    "    feature_dict['DTW Distance'] = fdtw[0]\n",
    "    \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_test_aug_03_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_a = X_test.copy()\n",
    "X_test_a['Augmented'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_augmented_03 = pd.DataFrame(X_test_aug_03_feature_list)\n",
    "X_test_augmented_03 = pd.concat([X_test_augmented_03, X_test_a], axis=0, ignore_index=True)\n",
    "X_test_augmented_03.to_pickle('/Users/matthewashman/github/MasterProject2018/EPDataAnalysis/Final Report/X_test_augmented_03.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_typical_response(row):\n",
    "    if (((row['Type'] + row['Patient']) == 'af8') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[2]\n",
    "    elif (((row['Type'] + row['Patient']) == 'af18') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avrt14') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avrt14') & (row['Channel'] == 'CS3-4')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avrt18')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[2]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt26') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[1]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt26') & (row['Channel'] != 'CS1-2')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[3]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt27')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt31') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt33') & (row['Channel'] == 'CS3-4')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[2]\n",
    "    elif (((row['Type'] + row['Patient']) == 'at1') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[4]\n",
    "    elif (((row['Type'] + row['Patient']) == 'at3') & (row['Channel'] == 'CS3-4')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt10') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[1]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt15') & (row['Channel'] == 'CS3-4')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt16') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avnrt1') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[1]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avrt13') & (row['Channel'] == 'CS1-2')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'avrt13')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'af14')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'af15')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'ep6') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[1]\n",
    "    elif (((row['Type'] + row['Patient']) == 'ep3') & (row['Channel'] == 'CS3-4')):\n",
    "        typical_response = S2_data[(S2_data['Type']==row['Type']) & \n",
    "                           (S2_data['Patient']==row['Patient']) &\n",
    "                           (S2_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    elif (((row['Type'] + row['Patient']) == 'af9') & (row['Channel'] == 'CS5-6')):\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                           (S1_data['Patient']==row['Patient']) &\n",
    "                           (S1_data['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[1]\n",
    "    else:\n",
    "        typical_response = S1_data[(S1_data['Type']==row['Type']) & \n",
    "                               (S1_data['Patient']==row['Patient']) &\n",
    "                               (S1_data['Channel']==row['Channel'])\n",
    "                               ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    return typical_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A shitty conduction delay detector\n",
    "def get_delay(x, amp_thresh=None, set_thresh=False):\n",
    "    if (set_thresh==True):\n",
    "        if any(abs(x)>amp_thresh):\n",
    "            return np.argmax(abs(x)>amp_thresh)\n",
    "        else:\n",
    "            return len(x)\n",
    "    else:    \n",
    "        return np.argmax(abs(x)>(max(abs(x))/2))\n",
    "    \n",
    "def denoise(x):\n",
    "    # Obtain Daubechies N=6 wavelet coefficients\n",
    "    waveletCoefs = pywt.wavedec(x, 'db7', mode='per')\n",
    "\n",
    "    # Throw away coefficients corresponding to noise\n",
    "    sigma = mad(waveletCoefs[-1])\n",
    "    uThresh = 1*sigma*np.sqrt(2*np.log(len(x)))\n",
    "    denoised = waveletCoefs[:]\n",
    "    denoised[1:] = (pywt._thresholding.hard(i, value=uThresh) for i in denoised[1:])\n",
    "\n",
    "    # Reconstruct the original signal\n",
    "    xDenoised = pywt.waverec(denoised, 'db7', mode='per')\n",
    "\n",
    "    return xDenoised\n",
    "\n",
    "def get_peaks(x, height_thresh, scale_amp=None, set_scale=False, plot = False):\n",
    "    x = np.array(x)\n",
    "    \n",
    "    # Get height_thresh\n",
    "    if set_scale:\n",
    "        height_thresh = height_thresh*scale_amp\n",
    "    else:\n",
    "        height_thresh = height_thresh*max(abs(x))\n",
    "    \n",
    "    # Denoise x\n",
    "    xdn = denoise(x)\n",
    "\n",
    "    # Detect peaks using detect_peaks\n",
    "    pos_peak_idx = detect_peaks(xdn, mph=height_thresh, threshold = 0)\n",
    "    neg_peak_idx = detect_peaks((-xdn), mph=height_thresh, threshold = 0)\n",
    "    peak_idx = np.concatenate([pos_peak_idx, neg_peak_idx])\n",
    "    peak_idx = np.sort(peak_idx)\n",
    "    # Edge indeces aren't detected\n",
    "    peak_idx = peak_idx[(peak_idx != 0) & (peak_idx != (len(xdn)-1))]\n",
    "\n",
    "    new_peak_idx = []\n",
    "    peak_amp = []\n",
    "    if (len(peak_idx) > 0):\n",
    "        new_peak_idx.append(peak_idx[0])\n",
    "        mp_thresh = 0.2*max(abs(x))\n",
    "        for i in range(len(peak_idx)-1):\n",
    "            idx = peak_idx[i]\n",
    "            idx_next = peak_idx[i+1]\n",
    "            mid_point = int((idx_next+idx)/2)\n",
    "            if (max([abs(x[idx_next]-x[mid_point]), abs(x[idx]-x[mid_point])]) > mp_thresh):\n",
    "                new_peak_idx.append(idx_next)\n",
    "\n",
    "        peak_idx = np.array(new_peak_idx)\n",
    "        peak_amp = x[peak_idx]\n",
    "\n",
    "    if plot == True:\n",
    "        fig, [ax1] = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(8,8))\n",
    "        ax1.plot(x, 'b' , xdn, 'r--', peak_idx, peak_amp, 'kx')\n",
    "        #plt.title(fileName)\n",
    "        ax1.set_xlabel('Sample')\n",
    "        ax1.set_ylabel('Normalised amplitude')\n",
    "        ax1.legend(['Original segment', 'Denoised segment', 'Detected peaks'])\n",
    "\n",
    "        plt.draw()\n",
    "        plt.waitforbuttonpress(0) # this will wait for indefinite time\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    return peak_idx, peak_amp\n",
    "\n",
    "def sample_entropy(U, m, r):\n",
    "\n",
    "    def _maxdist(x_i, x_j):\n",
    "        result = max([abs(ua-va) for ua, va in zip(x_i, x_j)])\n",
    "        return result\n",
    "\n",
    "    def _phi(m):\n",
    "        x = np.zeros([N,m-1])\n",
    "        for i in range(N-m+1):\n",
    "            x[i,:] = U[i:i+m-1]\n",
    "\n",
    "        C = 0\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                if i != j:\n",
    "                    if _maxdist(x[i,:], x[j,:]) <= r:\n",
    "                        C = C + 1\n",
    "\n",
    "        return C\n",
    "\n",
    "    U = U/max(abs(U))\n",
    "    N = len(U)\n",
    "\n",
    "    return -np.log(_phi(m+1)/_phi(m))\n",
    "\n",
    "def percentage_fractionation(x, peak_idxs, thresh=0.01, sr=1000):\n",
    "    # Get peak indexes and amplitude\n",
    "    peak_idx_diffs = np.diff(peak_idxs)\n",
    "    frac_time = 0\n",
    "    frac_time = np.sum(peak_idx_diffs[peak_idx_diffs < thresh*sr])\n",
    "    prcnt_frac = (frac_time/len(x))*100\n",
    "    return prcnt_frac\n",
    "\n",
    "def get_local_sample_entropy(x, centre_idx, width, m=2, r=0.05):\n",
    "    # Ensure width is odd\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return sample_entropy(x[:width+1], m, r)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return sample_entropy(x[len(x)-1-width:], m, r)\n",
    "    else:\n",
    "        return sample_entropy(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)], m, r)\n",
    "    \n",
    "def get_location_of_max_energy(x, M=14):\n",
    "    v = np.ones(M)\n",
    "    x_ = np.convolve(abs(x), v)\n",
    "    return (np.argmax(x_) + math.floor(M/2))\n",
    "        \n",
    "def get_local_peaks(x, centre_idx, width=25, height_thresh=0.1):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_peaks(x[:width+1], height_thresh)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_peaks(x[len(x)-1-width:], height_thresh)\n",
    "    else:\n",
    "        return get_peaks(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)], height_thresh)\n",
    "    \n",
    "def get_pse(x):\n",
    "    x_fft = np.fft.rfft(x)\n",
    "    x_P = (1/len(x_fft))*np.absolute(x_fft)**2\n",
    "    x_p = x_P/sum(x_P)\n",
    "    pse = np.sum([(-p*np.log2(p)) for p in x_p])\n",
    "    return pse\n",
    "\n",
    "def get_local_pse(x, centre_idx, width=50):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_pse(x[:width+1])\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_pse(x[len(x)-1-width:])\n",
    "    else:\n",
    "        return get_pse(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)])\n",
    "    \n",
    "def get_spectral_centroid(x):\n",
    "    x_fft = np.fft.rfft(x)\n",
    "    x_spectrum = np.absolute(x_fft)\n",
    "    normalized_spectrum = x_spectrum/sum(x_spectrum)\n",
    "    normalized_frequencies = np.arange(0, len(x_spectrum), 1)\n",
    "    return sum(normalized_frequencies * normalized_spectrum)\n",
    "\n",
    "def get_local_spectral_centroid(x, centre_idx, width=50):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_spectral_centroid(x[:width+1])\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_spectral_centroid(x[len(x)-1-width:])\n",
    "    else:\n",
    "        return get_spectral_centroid(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)])\n",
    "    \n",
    "def get_local_energy(x, centre_idx, width=60):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return np.sum(x[:width+1]**2)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return np.sum(x[len(x)-1-width:]**2)\n",
    "    else:\n",
    "        return np.sum(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)]**2)\n",
    "    \n",
    "def get_width_max_energy(x, M=14, width_thresh=0.2):\n",
    "    v = np.ones(M)\n",
    "    x_ = np.convolve(abs(x), v)\n",
    "    if any(x_[np.argmax(x_):] < width_thresh*np.max(x_)):\n",
    "        end_idx = np.argmax(x_) + np.argmax(x_[np.argmax(x_):] < width_thresh*np.max(x_))\n",
    "    else:\n",
    "        end_idx = len(x_)-1\n",
    "    if any(x_[np.argmax(x_)::-1] < width_thresh*np.max(x_)):  \n",
    "        start_idx = np.argmax(x_) - np.argmax(x_[np.argmax(x_)::-1] < width_thresh*np.max(x_))\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    return (end_idx - start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(x, col_prefix=''):\n",
    "    feature_dict = {}\n",
    "    height_thresh=0.1\n",
    "        \n",
    "    # Hand engineered features\n",
    "    peaks = get_peaks(x, height_thresh)\n",
    "    feature_dict[col_prefix + 'Number of Peaks'] = len(peaks[0])\n",
    "    feature_dict[col_prefix + 'Percentage Fractionation'] = percentage_fractionation(x, peaks[0], thresh=0.01)\n",
    "    \n",
    "    \n",
    "    max_energy_idx = get_location_of_max_energy(x)\n",
    "    feature_dict[col_prefix + 'Location of Maximum Energy'] = max_energy_idx\n",
    "    feature_dict[col_prefix + 'Sample Entropy Around Max Energy'] = get_local_sample_entropy(x, max_energy_idx, 30, m=3, r=0.15)\n",
    "    feature_dict[col_prefix + 'Width of Maximum Energy'] = get_width_max_energy(x, M=14, width_thresh=0.2)\n",
    "    \n",
    "    # Temporal features\n",
    "    feature_dict[col_prefix + 'Ratio Above 1xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 1)\n",
    "    feature_dict[col_prefix + 'Mean Absolute Value'] = np.mean(abs(x)/max(abs(x)))\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
